{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c7f19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import lightning as L\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from lightning.pytorch import seed_everything\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics.classification.f_beta import F1Score\n",
    "from torchmetrics import MetricCollection\n",
    "\n",
    "from mamkit.configs.base import ConfigKey\n",
    "from mamkit.configs.text import TransformerConfig\n",
    "from mamkit.data.collators import UnimodalCollator, TextTransformerCollator\n",
    "from mamkit.data.datasets import MMUSEDFallacy, InputMode\n",
    "from mamkit.data.processing import UnimodalProcessor\n",
    "from mamkit.models.text import Transformer\n",
    "from mamkit.utility.callbacks import PycharmProgressBar\n",
    "from mamkit.utility.model import MAMKitLightingModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d70cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "save_path = Path('results/mmused-fallacy/afc/text_only_bert')\n",
    "save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "base_data_path = Path('data')\n",
    "\n",
    "# Load dataset\n",
    "loader = MMUSEDFallacy(task_name='afc', input_mode=InputMode.TEXT_ONLY, base_data_path=base_data_path)\n",
    "\n",
    "# Load config\n",
    "config = TransformerConfig.from_config(\n",
    "    key=ConfigKey(dataset='mmused-fallacy', task_name='afc', input_mode=InputMode.TEXT_ONLY, tags={'anonymous', 'bert'})\n",
    ")\n",
    "\n",
    "# Training args\n",
    "trainer_args = {\n",
    "    'accelerator': 'auto',\n",
    "    'devices': 1,\n",
    "    'accumulate_grad_batches': 8,\n",
    "    'max_epochs': 3,\n",
    "}\n",
    "\n",
    "metrics = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12989768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Evaluation Loop\n",
    "for seed in config.seeds:\n",
    "    seed_everything(seed=seed)\n",
    "\n",
    "    for split_info in loader.get_splits(key='mm-argfallacy-2025'):\n",
    "        processor = UnimodalProcessor()\n",
    "        processor.fit(split_info.train)\n",
    "        split_info.train = processor(split_info.train)\n",
    "        split_info.val = processor(split_info.val)\n",
    "        split_info.test = processor(split_info.test)\n",
    "        processor.clear()\n",
    "\n",
    "        collator = UnimodalCollator(\n",
    "            features_collator=TextTransformerCollator(model_card=config.model_card, tokenizer_args=config.tokenizer_args),\n",
    "            label_collator=lambda labels: th.tensor(labels)\n",
    "        )\n",
    "\n",
    "        train_loader = DataLoader(split_info.train, batch_size=config.batch_size, shuffle=True, collate_fn=collator)\n",
    "        val_loader = DataLoader(split_info.val, batch_size=config.batch_size, shuffle=False, collate_fn=collator)\n",
    "        test_loader = DataLoader(split_info.test, batch_size=config.batch_size, shuffle=False, collate_fn=collator)\n",
    "\n",
    "        base_model = Transformer(\n",
    "            model_card=config.model_card,\n",
    "            is_transformer_trainable=config.is_transformer_trainable,\n",
    "            dropout_rate=config.dropout_rate,\n",
    "            head=config.head\n",
    "        )\n",
    "\n",
    "        model = MAMKitLightingModel(\n",
    "            model=base_model,\n",
    "            loss_function=config.loss_function,\n",
    "            num_classes=config.num_classes,\n",
    "            optimizer_class=config.optimizer,\n",
    "            val_metrics=MetricCollection({'f1': F1Score(task='multiclass', num_classes=6)}),\n",
    "            test_metrics=MetricCollection({'f1': F1Score(task='multiclass', num_classes=6)}),\n",
    "            **config.optimizer_args\n",
    "        )\n",
    "\n",
    "        trainer = L.Trainer(\n",
    "            **trainer_args,\n",
    "            callbacks=[\n",
    "                EarlyStopping(monitor='val_loss', mode='min', patience=5),\n",
    "                ModelCheckpoint(monitor='val_loss', mode='min'),\n",
    "                PycharmProgressBar()\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "        val_metrics = trainer.test(ckpt_path='best', dataloaders=val_loader)[0]\n",
    "        test_metrics = trainer.test(ckpt_path='best', dataloaders=test_loader)[0]\n",
    "\n",
    "        logging.info(f'Validation metrics: {val_metrics}')\n",
    "        logging.info(f'Test metrics: {test_metrics}')\n",
    "\n",
    "        for name, val in val_metrics.items():\n",
    "            metrics.setdefault('validation', {}).setdefault(name, []).append(val)\n",
    "        for name, val in test_metrics.items():\n",
    "            metrics.setdefault('test', {}).setdefault(name, []).append(val)\n",
    "\n",
    "        processor.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b984bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averaging Metrics\n",
    "for split in ['validation', 'test']:\n",
    "    for metric_name, values in metrics[split].items():\n",
    "        values_np = np.array(values).reshape(len(config.seeds), -1)\n",
    "        per_seed_avg = values_np.mean(axis=-1)\n",
    "        per_seed_std = values_np.std(axis=-1)\n",
    "        avg = per_seed_avg.mean()\n",
    "        std = per_seed_avg.std()\n",
    "\n",
    "        metrics[split][f'per_seed_avg_{metric_name}'] = (per_seed_avg, per_seed_std)\n",
    "        metrics[split][f'avg_{metric_name}'] = (avg, std)\n",
    "\n",
    "logging.info(metrics)\n",
    "np.save(save_path / 'metrics.npy', metrics)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
