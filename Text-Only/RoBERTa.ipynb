{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0709d763",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import lightning as L\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from lightning.pytorch import seed_everything\n",
    "from lightning.pytorch.callbacks import  ModelCheckpoint\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics.classification.f_beta import F1Score\n",
    "from torchmetrics import MetricCollection\n",
    "\n",
    "from mamkit.configs.base import ConfigKey\n",
    "from mamkit.configs.text import TransformerConfig\n",
    "from mamkit.data.collators import UnimodalCollator, TextTransformerCollator\n",
    "from mamkit.data.datasets import MMUSEDFallacy, InputMode\n",
    "from mamkit.data.processing import UnimodalProcessor\n",
    "from mamkit.models.text import Transformer\n",
    "from mamkit.utility.callbacks import PycharmProgressBar\n",
    "from mamkit.utility.model import MAMKitLightingModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0672e690",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "save_path = Path('results/mmused-fallacy/afc/text_only_RoBERTa')\n",
    "save_path.mkdir(parents=True, exist_ok=True)\n",
    "base_data_path = Path.cwd().joinpath('data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62590897",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loader = MMUSEDFallacy(task_name='afc',\n",
    "                       input_mode=InputMode.TEXT_ONLY,\n",
    "                       base_data_path=base_data_path)\n",
    "\n",
    "config = TransformerConfig.from_config(key=ConfigKey(dataset='mmused-fallacy',\n",
    "                                                     task_name='afc',\n",
    "                                                     input_mode=InputMode.TEXT_ONLY,\n",
    "                                                     tags={'anonymous', 'roberta'}))\n",
    "\n",
    "trainer_args = {\n",
    "    'accelerator': 'auto',\n",
    "    'devices': 1,\n",
    "    'batch_size': 8,\n",
    "    'max_epochs': 3,\n",
    "}\n",
    "\n",
    "\n",
    "config.optimizer = th.optim.Adam\n",
    "config.optimizer_args = {'lr': 2e-5}\n",
    "\n",
    "metrics = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c71c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for seed in config.seeds:\n",
    "    seed_everything(seed=seed)\n",
    "    for split_info in loader.get_splits(key='mm-argfallacy-2025'):\n",
    "        processor = UnimodalProcessor()\n",
    "        processor.fit(split_info.train)\n",
    "        split_info.train = processor(split_info.train)\n",
    "        split_info.val = processor(split_info.val)\n",
    "        split_info.test = processor(split_info.test)\n",
    "        processor.clear()\n",
    "\n",
    "        unimodal_collator = UnimodalCollator(\n",
    "            features_collator=TextTransformerCollator(model_card=config.model_card,\n",
    "                                                      tokenizer_args=config.tokenizer_args),\n",
    "            label_collator=lambda labels: th.tensor(labels)\n",
    "        )\n",
    "\n",
    "        train_loader = DataLoader(split_info.train, batch_size=config.batch_size, shuffle=True, collate_fn=unimodal_collator)\n",
    "        val_loader = DataLoader(split_info.val, batch_size=config.batch_size, shuffle=False, collate_fn=unimodal_collator)\n",
    "        test_loader = DataLoader(split_info.test, batch_size=config.batch_size, shuffle=False, collate_fn=unimodal_collator)\n",
    "\n",
    "        model = Transformer(model_card=config.model_card,\n",
    "                            is_transformer_trainable=config.is_transformer_trainable,\n",
    "                            head=config.head)\n",
    "        model = MAMKitLightingModel(model=model,\n",
    "                                    loss_function=config.loss_function,\n",
    "                                    num_classes=config.num_classes,\n",
    "                                    optimizer_class=config.optimizer,\n",
    "                                    val_metrics=MetricCollection({'f1': F1Score(task='multiclass', num_classes=6)}),\n",
    "                                    test_metrics=MetricCollection({'f1': F1Score(task='multiclass', num_classes=6)}),\n",
    "                                    **config.optimizer_args)\n",
    "\n",
    "        trainer = L.Trainer(**trainer_args,\n",
    "                            callbacks=[ModelCheckpoint(monitor='val_loss', mode='min'),\n",
    "                                       PycharmProgressBar()])\n",
    "        trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "        val_metrics = trainer.test(ckpt_path='best', dataloaders=val_loader)[0]\n",
    "        test_metrics = trainer.test(ckpt_path='best', dataloaders=test_loader)[0]\n",
    "        logging.info(f'Validation metrics: {val_metrics}')\n",
    "        logging.info(f'Test metrics: {test_metrics}')\n",
    "\n",
    "        for metric_name, metric_value in val_metrics.items():\n",
    "            metrics.setdefault('validation', {}).setdefault(metric_name, []).append(metric_value)\n",
    "        for metric_name, metric_value in test_metrics.items():\n",
    "            metrics.setdefault('test', {}).setdefault(metric_name, []).append(metric_value)\n",
    "        processor.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab1dd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metric_names = list(metrics['validation'].keys())\n",
    "for split_name in ['validation', 'test']:\n",
    "    for metric_name in metric_names:\n",
    "        metric_values = np.array(metrics[split_name][metric_name]).reshape(len(config.seeds), -1)\n",
    "        per_seed_avg = metric_values.mean(axis=-1)\n",
    "        per_seed_std = metric_values.std(axis=-1)\n",
    "        avg = per_seed_avg.mean(axis=-1)\n",
    "        std = per_seed_avg.std(axis=-1)\n",
    "        metrics[split_name][f'per_seed_avg_{metric_name}'] = (per_seed_avg, per_seed_std)\n",
    "        metrics[split_name][f'avg_{metric_name}'] = (avg, std)\n",
    "logging.info(metrics)\n",
    "np.save(save_path.joinpath('metrics.npy').as_posix(), metrics)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
